# Метод k-means. Ієрархічний кластерний аналіз

# Встановимо необхідні бібліотеки у створене раніше середовище conda:
# pip install yellowbrick
# conda install kneed

import warnings
import pickle
import numpy as np
import pandas as pd
from scipy.cluster.hierarchy import dendrogram
from matplotlib import pyplot as plt
import seaborn as sns
from yellowbrick.cluster import KElbowVisualizer
from kneed import KneeLocator
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# %%

# Застосування методів кластеризації розглянемо на мікронаборі даних NCI-60. Цей набір описує групу ліній ракових клітин 
# людини й використовується Національним інститутом раку для відбору сполук із потенційною протираковою активністю.

# У наборі міститься інформація про 6830 вимірів експресії генів на 64 лініях клітин різних видів раку: 
# лейкемія, меланома, рак легенів, мозку, яєчників, молочної залози, товстої кишки, нирок і простати. 
# Кожна лінія позначена типом раку, який вона представляє.

# Завантажимо набір даних зі словника:
with open('../datasets/mod_06_topic_12_nci_data.pkl', 'rb') as fl:
    data_dict = pickle.load(fl)

data = data_dict['data']
target = data_dict['labels']

data.shape 

# %%
# Маємо 64 рядки і 6830 стовпчиків. Окремо доступна інформація про типи раку, яким відповідають представлені в наборі лінії клітин.
# Ми не будемо використовувати цю інформацію при виконанні PCA та кластеризації. 
# Але після отримання результатів перевіримо, як визначені нами кластери співвідносяться з оригінальними мітками в наборі.

# Кількість міток за типами в наборі:
target['label'].value_counts().sort_index()

# %%
# Для навчання без учителя не треба розбивати набір на тренувальну і тестову вибірки. 
# Одразу переходимо до нормалізації даних і розрахунку головних компонент, як в темі “Зменшення розмірності даних. Метод PCA”.
X = StandardScaler().fit_transform(data)

pca = PCA(random_state=42).fit(X)
pve = pca.explained_variance_ratio_

# %%

# На перший погляд здається, що розрахунок більшої кількості компонент допоможе зберегти більше "інформативності" даних, 
# пояснюючи більший відсоток їх змінності. Проте на практиці наступає момент, коли додаткові компоненти призводять до 
# невиправданого збільшення розмірності, і тоді описане вище “правило ліктя" також можна використовувати, щоб визначити, коли зупинитися.

# На графіку дисперсії (змінності) даних, поясненої за допомогою кожної наступної головної компоненти, можна спробувати знайти “лікоть” 
# так само, як і на графіку залежності сумарної міри щільності від заданої кількості кластерів.
# Для пошуку “ліктя” на графіку дисперсії (змінності) даних, поясненої головними компонентами, використаємо функціонал пакета kneed:

sns.set_theme()

kneedle = KneeLocator(
    x=range(1, len(pve) + 1),
    y=pve,
    curve='convex',
    direction='decreasing')

kneedle.plot_knee()

plt.show()

# %%
# Визначена візуально за “правилом ліктя” оптимальна кількість компонент дорівнює 9.
# Визначимо й візуалізуємо на графіку кумулятивну частку змінності даних, яку пояснюють 9 перших головних компонент:

n_components = kneedle.elbow

ax = sns.lineplot(np.cumsum(pve))

ax.axvline(x=n_components,
           c='black',
           linestyle='--',
           linewidth=0.75)

ax.axhline(y=np.cumsum(pve)[n_components],
           c='black',
           linestyle='--',
           linewidth=0.75)

ax.set(xlabel='number of components',
       ylabel='cumulative explained variance')

plt.show()

# %%
# Отже, після зменшення розмірності даних ми зберігаємо приблизно 44% “інформативності” вхідного набору (кумулятивна частка 
# поясненої дисперсії на графіку дорівнює ~0.44). Це може здатися незначним, проте, враховуючи малу кількість об'єктів у наборі, 
# треба суттєво обмежити кількість ознак, які ми можемо ефективно використовувати для подальшого аналізу.

# Зменшуємо розмірність даних за допомогою PCA:
X = pca.transform(X)[:, :n_components]

# %%
# Кластеризація набору даних. Визначення кількості кластерів за допомогою KMeans

# Далі виконаємо кілька варіантів кластеризації, використовуючи трансформований набір даних і задаючи різну кількість кластерів для пошуку.
# На кожному етапі обчислимо сумарну міру щільності та побудуємо графік її залежності від заданої кількості кластерів. 
# Потім за допомогою "правила ліктя" визначимо оптимальну кількість кластерів. (за допомогою функціоналу пакета yellowbrick)

model_kmn = KMeans(random_state=42)

visualizer = KElbowVisualizer(
    model_kmn,
    k=(2, 10),
    timings=False)


with warnings.catch_warnings():
    warnings.simplefilter('ignore')

    visualizer.fit(X)

visualizer.show()
# Визначена оптимальна кількість кластерів для цього набору даних за “правилом ліктя” дорівнює 5.

# %%

# Ієрархія кластерів на основі AgglomerativeClustering

  # Наступним кроком буде спроба отримати альтернативні кластери на нашому наборі даних за допомогою агломеративної кластеризації.
  # Для візуалізації отриманих результатів ми створимо допоміжну функцію, яка будуватиме дендрограму на основі обрахунків моделі. 
  # Для побудови дендрограми ця функція використовуватиме дані про злиття кластерів (атрибут моделі .children_), 
  # відстані між ними та кількість об'єктів у кожному злитті.

# У результаті ми отримаємо графічне зображення ієрархічної структури кластерів, яке допоможе нам краще зрозуміти зв'язки 
# та розподіл об'єктів у наборі даних.

def plot_dendrogram(model, **kwargs):

    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for x, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[x] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    dendrogram(linkage_matrix, **kwargs)


# %%
# Проведемо кластеризацію набору даних за допомогою методу AgglomerativeClustering із пакета sklearn. 
# Щоб отримати повне дерево, ми встановлюємо гіперпараметр distance_threshold=0.

model_agg = AgglomerativeClustering(
    distance_threshold=0,
    n_clusters=None)

model_agg = model_agg.fit(X)

# %%
# Передамо модель у підготовлену нами функцію й побудуємо дендрограму:

    plot_dendrogram(model_agg, truncate_mode='level', p=3)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Number of points in node (or index of point if no parenthesis)')

plt.show()

# Аналізуючи дендрограму, можна визначити оптимальну кількість кластерів у наборі даних відповідно до наших дослідницьких цілей. 
# Але оскільки це навчання без учителя, то немає правильної відповіді на питання, чи буде обрана кількість кластерів достатньою, чи ні.

# %%

# Отримання міток кластерів
# Для експерименту проведемо кластеризацію обома методами (KMeans і Agglomerative Clustering), щоб отримати однакову задану кількість кластерів. 
# Для цього при створенні екземплярів обох об’єктів укажемо кількість кластерів (5), визначену раніше за "правилом ліктя".

k_best = visualizer.elbow_value_

with warnings.catch_warnings():
    warnings.simplefilter('ignore')

    model_kmn = KMeans(n_clusters=k_best, random_state=42).fit(X)
    model_agg = AgglomerativeClustering(n_clusters=k_best).fit(X)

labels_kmn = pd.Series(model_kmn.labels_, name='k-means')
labels_agg = pd.Series(model_agg.labels_, name='h-clust')

# %%
# Тепер можемо порівняти кластери, які були отримані різними методами, шляхом підрахунку кількості унікальних об'єктів на їх перетині:
pd.crosstab(labels_agg, labels_kmn)
# Наприклад, кластер 0 ієрархічної кластеризації, відповідає кластеру 2 k-середніх. 
# Кількість унікальних об'єктів, які знаходяться одночасно у цих кластерах (на їх перетині), 
# відповідає кількості об'єктів у кожному із кластерів окремо (у відповідному рядку чи стовпчику)

# Отже, результати кластеризації, проведеної різними методами, виявилися ідентичними, 
# а мітки, які використовуються для позначення кластерів, є довільними. 
# Іншими словами, заміна міток кластерів не змінить результатів кластеризації.


# %%
# Подивимось, як співвідносяться кластери з типами ракових клітин у наборі:
pd.crosstab(target['label'], labels_kmn)

# Природно, що лінії клітин одного типу переважно потрапили в один кластер, проте розподіл за визначеними нами кластерами не є ідеальним.

# %%
# Візуалізація результатів кластеризації
# Щоб візуалізувати результати кластеризації у двовимірному просторі, 
# можемо використати розраховані нами головні компоненти, як ми вже це робили в темі “Зменшення розмірності даних”

fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 5), sharey=True)

for i, s in enumerate([target['label'], labels_kmn]):
    ax = axes[i]

    sns.scatterplot(x=X[:, 0],
                    y=X[:, 1],
                    hue=s,
                    style=s,
                    edgecolor='black',
                    linewidth=0.5,
                    s=60,
                    palette='tab20',
                    legend=False,
                    ax=ax)

    ax.set(title=s.name)

plt.show()

# Для детального аналізу результатів кластеризації, статистичного опису кластерів та глибшого розуміння причин, 
# що призвели до групування об'єктів в кластери, які визначила модель, потрібні додаткові дані і спеціальні знання в галузі медицини.
