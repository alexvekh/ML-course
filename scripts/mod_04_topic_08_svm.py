import warnings
import pandas as pd
import numpy as np
import category_encoders as ce
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix

# %%
# Для побудови SVM-класифікатора використовувуємо адаптований набір даних PetFinder і спрогнозуємо за даними 
# профілю тварини на сайті, чи зможе вона знайти у короткий термін нового власника.

# Завантажимо дані та розглянемо, які ознаки доступні для використання в моделі.
data = pd.read_csv('../datasets/mod_04_topic_08_petfinder_data.csv.gz')
data.info()
  # Маємо набір із 14 (11 категоріальних і 3 числових) ознак і цільової змінної AdoptionSpeed
  # PhotoAmt — загальна кількість фотографій у профілі тварини на сайті
# %%
# Перевіримо кількість унікальних значень у вхідних ознаках. 
# Звернемо увагу на ознаку Description (кожне значення є унікальним) та цільову змінну AdoptionSpeed (має 5 категорій).
data.nunique()

# %%
# Адаптація ознак і цільової змінної
# Description — це супроводжувальний текст (опис) у профілі тварини на сайті.
data['Description'].head()

# %%
#  sentiment analysis виходить за межі цього курсу. Тому видаляємо цю ознаку.
data.drop('Description', axis=1, inplace=True)

# %%
# Для цільової змінної маємо п'ять унікальних значень (категорій). 
# Категорії 0-3 представляють відгук нового власника на профіль тварини у термін до 90 днів після дати розміщення профілю на сайті. 
# Категорія 4 вказує на те, що тварина так і не знайшла нового власника після 90+ днів, починаючи з дати розміщення профілю.
data['AdoptionSpeed'].value_counts().sort_index()

# %%
# Модифікуємо цільову змінну під задачу бінарної класифікації: 
  # 0 — тварина не знайшла власника (у прийнятний термін до 90 днів), 
  # 1 — тварина знайшла власника.
data['AdoptionSpeed'] = np.where(data['AdoptionSpeed'] == 4, 0, 1)   # 0—якщо умова виконується, 1—якщо ні
data['AdoptionSpeed'].value_counts()

# %%
# Ознака Fee має числове значення — це розмір плати, яка може зніматися з нового власника за прихисток тварини. 
# Для подальшого використання ми перетворимо її на бінарний показник: чи знімається оплата, чи ні.
data['Fee'] = data['Fee'].astype(bool).astype(int).astype(str)
    # Спочатку в булевий тип (True або False) - ненульові (> 0) = True, а нульові (0) = False.
    # Потім булеві на цілі числа: True стає 1. False стає 0.
    # Потім цілі (1 або 0) на рядковий тип ("1" або "0").
# %%
# Розбиття на тренувальну і тестову вибірки
X_train, X_test, y_train, y_test = (
    train_test_split(
        data.drop('AdoptionSpeed', axis=1),
        data['AdoptionSpeed'],
        test_size=0.2,
        random_state=42))

# %%

# Після перетворення ознаки Fee на бінарну залишилося тільки дві числові ознаки: Age і PhotoAmt. 
# Обидві мають невелику кількість унікальних значень. 
# Можна перетворити їх на дискретні - розбити їх неперервні значення на певну кількість корзин (визначених інтервалів) (як в темі “Дослідницький аналіз даних (EDA)”.

# Для зручності можна використовувати об'єкт KBinsDiscretizer із пакета sklearn. 
# Створюємо екземпляр KBinsDiscretizer з необхідними налаштуваннями, навчаємо його на тренувальному наборі даних (алгоритм визначає інтервали корзин). 
# Потім використовуємо екземпляр об’єкта для перетворення визначених ознак на категоріальні у тренувальному й тестовому наборах.

# Цей метод перетворення (дискретизація) може покращити здатність моделі до узагальнення, оскільки зменшує зашумленість вхідних даних.

num_cols = X_train.select_dtypes(exclude='object').columns

kbins = KBinsDiscretizer(encode='ordinal').fit(X_train[num_cols])

X_train[num_cols] = (kbins
                     .transform(
                         X_train[num_cols])
                     .astype(int)
                     .astype(str))

X_test[num_cols] = (kbins
                    .transform(
                        X_test[num_cols])
                    .astype(int)
                    .astype(str))

# %%
# Кодування категоріальних змінних

# Всі ознаки стали категоріальними, тож тепер потрібно їх закодувати для подальшого використання в моделі. 
# Цього разу спробуємо застосувати об’єкт TargetEncoder із пакета category_encoders.
# TargetEncoder кожну категорію ознаки кодує з урахуванням глобального середнього значення цільової змінної 
# та її середнього значення на підмножині, яка формується вибором категорії. 
# Цей підхід нагадує розрахунок умовних імовірностей, як в темі “Байєсівська класифікація. Метод kNN”.

encoder = ce.TargetEncoder()

X_train = encoder.fit_transform(X_train, y_train)
X_test = encoder.transform(X_test)

X_train.head()

# Загалом потреби в кодуванні і наступній нормалізації бінарних ознак (Type, Gender, Fee) немає. 
# Однак, у даному випадку додаткова обробка цих ознак не вплине на якість моделі.
# Тому, щоб зменшити обсяг “ручного” коду, ми вирішили не виключати їх із загального “пакетного” процесу підготовки даних 
# (тобто не будемо виконувати попереднє виключенням цих ознак на окремих етапах обробки та наступнє приєднання до набору).

# %%
# Нормалізація змінних

scaler = StandardScaler().set_output(transform='pandas')

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# %%
# Побудова моделі (будуємо SVМ-класифікатор за допомогою об'єкта SVC бібліотеки sklearn).

# Враховуючи дисбаланс класів (під час EDA встановлено, що клас 1 має перевагу), встановлюємо параметр class_weight='balanced'. 
# Також обираємо ядерну функцію, наприклад, поліноміальну, встановлюючи параметр kernel='poly'. 
# Оскільки модель буде класифіккувати без розрахунку ймовірностей віднесення об'єктів до класів, ми вмикаємо параметр probability=True.
# Це дозволяє “вбудувати” в модель додаткові алгоритми під час навчання, які відповідатимуть за розрахунок імовірностей і забезпечать нам бажану функціональність.

clf = SVC(class_weight='balanced',
          kernel='poly',
          probability=True,
          random_state=42)

clf.fit(X_train, y_train)

# %%
# За допомогою навченої моделі отримуємо прогнози для тестового набору, будуємо матрицю спряженості (confusion_matrix).

preds = clf.predict(X_test)

confusion_matrix(y_test, preds)

# %%
# Наша модель правильно спрогнозувала більшість об'єктів для обох класів (відображені на головній діагоналі confusion_matrix). 
# Можемо сказати, що точність моделі є "доброю" (~71%) — це ми визначили за допомогою простої метрики accuracy, 
# оскільки клас 1, який нас цікавить, переважає у вхідних даних.

print(f'Model accuracy is: {accuracy_score(y_test, preds):.1%}')

# %%

# Для покращення моделі можемо додати додаткові ознаки, які можуть бути створені з тексту та зображень, доступних у профілях тварин 
# на сайті, або провести експерименти з кодуванням вхідних даних або вибором параметрів (ядерної функції) SVM-класифікатора.

# Приклад використання моделі
# Завдяки ймовірностям, які ми ввімкнули, класифікатор може не лише прогнозувати клас, але й оцінювати ймовірність того, 
# чи знайде тварина нового власника. 
# Так можна представити модель як додатковий сервіс для користувачів, які розміщують профілі тварин на сайті.
# Для демонстрації того, як це може працювати, створимо (опишемо вхідними ознаками) новий профіль тварини.

pet = pd.DataFrame(
    data={
        'Type': 'Cat',
        'Age': 7,
        'Breed1': 'Tabby',
        'Gender': 'Male',
        'Color1': 'Black',
        'Color2': 'White',
        'MaturitySize': 'Small',
        'FurLength': 'Short',
        'Vaccinated': 'No',
        'Sterilized': 'No',
        'Health': 'Healthy',
        'Fee': True,
        'PhotoAmt': 2,
    },
    index=[0])

# %%

# Перед уведенням його в модель необхідно застосувати всі ті ж самі перетворення змінних і в тій самій послідовності. 
# Для трансформації вхідних ознак використовуємо навчені екземпляри відповідних об’єктів (kbins, scaler, encoder).
# Тепер ми можемо розрахувати ймовірність знаходження твариною нового власника та відобразити цю інформацію користувачу, який створив профіль. 

pet[num_cols] = kbins.transform(pet[num_cols]).astype(int).astype(str)

with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    prob = (clf
            .predict_proba(
                scaler
                .transform(
                    encoder
                    .transform(
                        pet)))
            .flatten())

print(f'This pet has a {prob[1]:.1%} probability "of getting adopted"')

# Змінюючи параметри профілю тварини, ми можемо помітити, як змінюється розрахована ймовірність і як вона залежить від різних параметрів, таких як вік тварини, стан її здоров’я, кількість фотографій у профілі тощо.
