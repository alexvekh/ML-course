# Практика застосування PCA. 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.ensemble import GradientBoostingClassifier

# %%
# EDA датасету Breast Cancer Wisconsin

# Використання методу PCA для зменшення розмірності даних. 
# Цей набір містить ознаки, що базуються на цифрових зображеннях ядер клітин із молочної залози.

# У рамках курсу Numerical Programming ми використовували цей набір для розбору математичної онови алгоритмів зменшення розмірності,
# а тепер заглибимося в особливості підготовки даних і визначення оптимальної кількості компонент для PCA у практичних задачах.

# Для масиву клітин на кожному зображенні було розраховано середні значення, стандартні відхилення й “найгірші” (мін/макс) за 10 ознаками. 
# У результаті кожне зображення має 30 ознак. Основна мета у класифікації клітин на "злоякісні" (клас 0) та "доброякісні" (1), 
# що робить це задачею бінарної класифікації.

# завантажимо дані й поглянемо на ознаки. Розібратися в ознаках важко, проте наразі не обов'язково. 
# Наша мета — побудувати класифікатор, скориставшись PCA для зменшення розмірності вхідного простору ознак. 
# Це означає, що ми перейдемо у простір, який вже не матиме прямого зв'язку з "фізичними” властивостями початкових ознак.

data, target = load_breast_cancer(return_X_y=True, as_frame=True)
data.head()

# %%
# 1. переглянемо типи вхідних ознак і переконаємося у відсутності пропусків у даних.
data.info()
   # всі ознаки числові. Усього 569 спостережень і 30 ознак.
# %%
# Правило 1:10 означає, що на кожні 10 спостережень найменш представленого класу в наборі даних ми можемо ефективно оцінити 1 
# ознаку за допомогою базових алгоритмів машинного навчання (наприклад, логістичної регресії).
# Розподіл кількості об’єктів за класами: 1: 357, 0: 212
# У наборі 200+ спостережень класу (менш поширеного), (відповідно до правила 1:10), можемо оцінити приблизно 20 параметрів.
# отже, зменшення розмірності набору перед навчанням класифікатора може бути цілком виправданим.
 
target.value_counts()

# %%
# Очистка від викидів (метод PCA чутливий до аномалій у даних, Отже, передPCA, очистимо дані від викидів. (z-критерій)

# розрахуємо z-критерій для кожного об'єкта за кожною ознакою та перевіримо, чи значення критерію в діапазоні ±3 стандартні відхилення. 
# Потім встановимо правило: якщо значення z-критерію для 20% і більше ознак об'єкта (тобто для 6 з 30 ознак) поза межами ±3 стандартних відхилень, 
# то такий об'єкт буде визнаний викидом, і його буде видалено з набору даних.
out = (data
       .apply(lambda x:
              np.abs(zscore(x))
              .ge(3))
       .astype(int)
       .mean(1))

out_ind = np.where(out > 0.2)[0]

data.drop(out_ind, inplace=True)
target.drop(out_ind, inplace=True)

# Отже, ми видалили з набору даних 7 спостережень, визнаних аномальними за наведеним простим евристичним правилом.

# %%

data.shape

# %%
# Розбиття на тренувальну і тестову вибірки
X_train, X_test, y_train, y_test = (
    train_test_split(
        data,
        target,
        test_size=0.2,
        random_state=42))

# %%
# Нормалізація змінних (суттєво покращує результати методу PCA, який ми плануємо використати)

scaler = StandardScaler().set_output(transform='pandas')

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# %%
# Зменшення розмірності даних
  # Виконуємо розрахунок головних компонент для нашого набору даних, використовуючи об’єкт PCA пакета 

pca = PCA().set_output(transform='pandas').fit(X_train)

# %%
# Для ефективного використання PCA важливо вміти визначати, скільки компонент потрібно для опису даних. 
# Це можна зробити, розглянувши графік кумулятивної дисперсії (змінності) даних, 
# поясненої за допомогою визначеної кількості головних компонент (ознак у просторі відповідної розмірності).

# Наприклад, перші п'ять компонент нашого набору даних можуть пояснити близько 90% дисперсії, а для опису майже 100% дисперсії 
# потрібно близько 20 компонент. (Можемо бачити, який рівень “надлишкової” складності можуть мати багатовимірні дані.)

# Для визначення оптимальної кількості головних компонент спочатку задаємо бажаний рівень "залишкової інформативності" наприклад, 85%.
# Потім обчислюємо кумулятивну суму поясненої дисперсії за допомогою np.cumsum(pca.explained_variance_ratio_) і визначаємо індекс 
# елемента цього масиву, який має значення рівне або більше заданого порога, використовуючи np.searchsorted(explained_variance, 0.85).

sns.set_theme()

explained_variance = np.cumsum(pca.explained_variance_ratio_)

ax = sns.lineplot(explained_variance)
ax.set(xlabel='number of components',
       ylabel='cumulative explained variance')

n_components = np.searchsorted(explained_variance, 0.85)

ax.axvline(x=n_components,
           c='black',
           linestyle='--',
           linewidth=0.75)

ax.axhline(y=explained_variance[n_components],
           c='black',
           linestyle='--',
           linewidth=0.75)

plt.show()

# %%
# Трансформуємо тренувальну і тестову вибірки за допомогою PCA
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# %%
# так виглядатиме датасет, який ми будемо використовувати для навчання моделі (після зменшення розмірності даних із 30 до 5 ознак).
X_train_pca.iloc[:, :n_components].head()

# %%
# Візуалізація вхідного набору даних
# Використавши перші три компоненти, можемо візуалізувати вхідний набір даних у тривимірному просторі. 
# Бачимо, як добре розділені класи.
plt.figure(figsize=(8, 8))

ax = plt.subplot(projection='3d')

ax.scatter3D(
    X_train_pca.iloc[:, 0],
    X_train_pca.iloc[:, 1],
    X_train_pca.iloc[:, 2],
    c=y_train,
    s=20,
    cmap='autumn',
    ec='black',
    lw=0.75)

ax.view_init(elev=30, azim=30)

plt.show()

# %%
# Побудова й оцінка моделі
# Тепер побудуємо класифікатор (ансамбль класифікаторів) GradientBoostingClassifier на повному наборі даних і оцінимо його точність, 
# щоб пізніше порівняти з точністю прогнозів моделі, навченої на даних меншої розмірності.


clf_full = GradientBoostingClassifier()

clf_full.fit(X_train, y_train)

pred_full = clf_full.predict(X_test)

score_full = accuracy_score(y_test, pred_full)

print(f'Model accuracy: {score_full:.1%}')
# Маємо рівень точності приблизно у 93,8%.

# %%
# Тепер створимо аналогічний класифікатор, проте навчимо його на наборі даних зі зменшеною розмірністю (5 головних компонент):
clf_pca = GradientBoostingClassifier()

clf_pca.fit(X_train_pca.iloc[:, :n_components], y_train)

pred_pca = clf_pca.predict(X_test_pca.iloc[:, :n_components])

score_pca = accuracy_score(y_test, pred_pca)

print(f'Model accuracy (PCA): {score_pca:.1%}')
# Як бачимо, точність такого класифікатора становить 96,5%. 
# Це означає, що його здатність до узагальнення навіть більша, ніж у моделі, навченій на всіх вхідних ознаках.

# %%
# Важливі ознаки й головні компоненти
# За допомогою класифікатора, навченого на вхідному наборі даних, можемо визначити важливість ознак, як в темі “Дерева рішень”.
plt.figure(figsize=(3, 8))

(pd.Series(
    data=clf_full.feature_importances_,
    index=X_train.columns)
    .sort_values(ascending=True)
    .plot
    .barh())

plt.show()
# Як бачимо, ця модель також виділила 5 ознак як такі, що найбільше вплинули на прийняття нею рішення щодо визначення класів. 
# Проте ми не можемо стверджувати, що ці ознаки і є нашими головними компонентами, 
# оскільки кожна з головних компонент фактично є комбінацією багатьох вхідних ознак.
